# World Cup Data Pipeline

This project implements a modern ELT (Extract, Load, Transform) data pipeline for FIFA World Cup 2022 data. It leverages Apache Airflow for orchestration, Docker for containerization, PostgreSQL for staging, and Azure services (Data Lake, Blob Storage, Data Factory, and Synapse SQL) for scalable data processing and storage. The pipeline culminates in visualizations using Google Looker Studio.

## üöÄ Overview

- **Data Sources**: Raw datasets related to FIFA World Cup 2022 matches, teams, and players.
- **Orchestration**: Apache Airflow manages the workflow, ensuring timely and orderly execution of tasks.
- **Containerization**: Docker encapsulates the environment, promoting consistency across development and production.
- **Staging**: PostgreSQL serves as the initial landing zone for raw data ingestion.
- **Cloud Storage**: Azure Blob Storage and Azure Data Lake Gen2 store raw and processed data, respectively.
- **Data Processing**: Pandas handles data cleaning and transformation tasks.
- **Data Movement**: Azure Data Factory orchestrates the transfer of data between services.
- **Data Warehousing**: Azure Synapse SQL provides a platform for querying and analyzing the transformed data.
- **Visualization**: Google Looker Studio presents insights through interactive dashboards.

## üõ†Ô∏è Technologies Used

- **Apache Airflow** ‚Äì Workflow orchestration
- **Docker** ‚Äì Containerisation
- **PostgreSQL** ‚Äì Storing metadata on the airflow orchastrator
- **Azure Blob Storage** ‚Äì Cloud storage for raw files
- **Azure Data Lake Gen2** ‚Äì Storage for processed data
- **Azure Data Factory** ‚Äì Data movement and ETL orchestration
- **Azure Synapse SQL** ‚Äì Analytical SQL engine
- **Pandas** ‚Äì Data manipulation
- **Google Looker Studio** ‚Äì Data visualization

# Dashboard
- ![Dashboard Preview](./dashboard_1.png)
- ![Dashboard Preview 2](dashboard_2.png)
